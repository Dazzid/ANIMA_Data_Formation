"""
05_midi_mpe_tokenization.py
===========================
Tokenization module for 53-TET MPE MIDI files.

Converts MIDI Polyphonic Expression (MPE) files into flat token sequences 
suitable for training a GPT-2 model. Each chord event is decomposed into 
a series of sub-tokens representing timing, pitch (in 53-TET steps), and velocity.

Token Vocabulary
----------------
The token vocabulary consists of 5 categories:

  1. SPECIAL TOKENS:
     <pad>, <start>, <end>, <sep>

  2. STRUCTURAL TOKENS:
     CHORD_START, CHORD_END, BAR

  3. DURATION TOKENS:
     DUR_1.0, DUR_1.33, DUR_1.5, DUR_2.0, DUR_2.67, DUR_3.0, DUR_4.0, 
     DUR_6.0, DUR_8.0 ... (quantized to the grid values used in the dataset)

  4. PITCH TOKENS:
     P_<step> where step is the absolute 53-TET pitch (0 to 529).
     Octave 0 = steps 0-52, Octave 1 = 53-105, etc.
     Practical range in the dataset: ~P_130 to P_370.

  5. VELOCITY TOKENS:
     V_<bin> where bin is a quantized velocity level (1-8).

Sequence Format
---------------
A tokenized song looks like:

  <start>  CHORD_START DUR_4.0 P_212 V_3 P_243 V_3 P_265 V_2 P_284 V_3 P_306 V_2 CHORD_END  BAR  CHORD_START DUR_4.0 ...  <end>

The model learns:
  - Which pitches form musically coherent chords (microtonal voicings)
  - How chords progress over time (harmonic rhythm)
  - Stylistic patterns of 53-TET harmony

Usage
-----
  from 05_midi_mpe_tokenization import MPETokenizer

  tokenizer = MPETokenizer()
  tokens = tokenizer.encode_file("path/to/file.mid")
  midi_events = tokenizer.decode(tokens)
  tokenizer.save_vocab("vocab.json")
"""

import json
import mido
import math
import os
import random
from pathlib import Path
from collections import Counter


# =============================================================================
# CONSTANTS
# =============================================================================

# 53-TET divisions per octave
TET_53 = 53

# Pitch Bend Range (set via RPN in the MIDI files) = ±2 semitones = ±200 cents
PB_RANGE_CENTS = 200.0

# MIDI pitch bend limits
PB_MAX = 8191
PB_MIN = -8192

# Maximum number of notes per chord 
# (Dataset is mostly 5, sometimes 6. Pad to MAX_CHORD_NOTES)
MAX_CHORD_NOTES = 8

# Maximum 53-TET pitch we tokenize (10 octaves × 53 = 530)
MAX_53TET_STEP = 530

# Velocity quantization bins (1-based: V_1 through V_8)
NUM_VELOCITY_BINS = 8

# Duration quantization grid (in beats)
# Derived from dataset analysis — covers all common rhythmic values
DURATION_GRID = [
    0.5, 0.67, 0.75, 1.0, 1.33, 1.5, 1.6, 
    2.0, 2.67, 3.0, 3.2, 4.0, 4.5, 4.8, 
    5.33, 6.0, 8.0
]

# Special tokens
PAD_TOKEN = "<pad>"
START_TOKEN = "<start>"
END_TOKEN = "<end>"
SEP_TOKEN = "<sep>"

# Structural tokens
CHORD_START_TOKEN = "CHORD_START"
CHORD_END_TOKEN = "CHORD_END"
BAR_TOKEN = "BAR"
REST_TOKEN = "REST"


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def midi_bend_to_53tet_step(midi_note, pitch_bend_value):
    """
    Convert a MIDI note + pitch bend (MPE) to an absolute 53-TET step.
    
    The MPE encoding uses per-channel pitch bend with ±200 cents range
    to shift each note to its correct 53-TET pitch.
    
    Args:
        midi_note (int): MIDI note number (0-127)
        pitch_bend_value (int): MIDI pitch bend (-8192 to 8191)
    
    Returns:
        int: Absolute 53-TET step number
    """
    # Convert pitch bend to cents
    cents = (pitch_bend_value / PB_MAX) * PB_RANGE_CENTS
    
    # MIDI note in 53-TET (without bend): note * 53/12
    base_step = midi_note * TET_53 / 12.0
    
    # Add the microtonal deviation: cents / (cents_per_53tet_step)
    # 1 octave = 1200 cents = 53 steps → 1 step = 1200/53 ≈ 22.64 cents
    deviation_steps = cents * TET_53 / 1200.0
    
    return round(base_step + deviation_steps)


def quantize_duration(duration_beats):
    """
    Snap a duration (in beats) to the nearest value in the grid.
    
    Args:
        duration_beats (float): Duration in beats
    
    Returns:
        float: Quantized duration from DURATION_GRID
    """
    if duration_beats <= 0:
        return DURATION_GRID[0]
    
    best = DURATION_GRID[0]
    best_dist = abs(duration_beats - best)
    
    for g in DURATION_GRID:
        dist = abs(duration_beats - g)
        if dist < best_dist:
            best = g
            best_dist = dist
    
    return best


def quantize_velocity(velocity, num_bins=NUM_VELOCITY_BINS):
    """
    Quantize MIDI velocity (0-127) into bins (1 to num_bins).
    
    Args:
        velocity (int): MIDI velocity 0-127
        num_bins (int): Number of output bins
    
    Returns:
        int: Velocity bin (1-based)
    """
    if velocity <= 0:
        return 1
    # Map 1-127 → 1-num_bins
    bin_idx = math.ceil(velocity / 127.0 * num_bins)
    return max(1, min(num_bins, bin_idx))


def dequantize_velocity(vel_bin, num_bins=NUM_VELOCITY_BINS):
    """
    Convert a velocity bin back to a MIDI velocity value.
    
    Args:
        vel_bin (int): Velocity bin (1-based)
        num_bins (int): Number of bins
    
    Returns:
        int: MIDI velocity (1-127)
    """
    return max(1, min(127, round(vel_bin / num_bins * 127)))


def step53_to_midi_and_bend(step_53):
    """
    Convert a 53-TET step back to MIDI note + pitch bend.
    
    Args:
        step_53 (int): Absolute 53-TET step
    
    Returns:
        tuple: (midi_note, pitch_bend_value)
    """
    # Closest 12-TET MIDI note
    midi_note = round(step_53 * 12 / TET_53)
    midi_note = max(0, min(127, midi_note))
    
    # Residual in 53-TET steps
    expected_step = midi_note * TET_53 / 12.0
    residual_steps = step_53 - expected_step
    
    # Convert residual to cents, then to pitch bend
    residual_cents = residual_steps * 1200.0 / TET_53
    pitch_bend = int(round(residual_cents / PB_RANGE_CENTS * PB_MAX))
    pitch_bend = max(PB_MIN, min(PB_MAX, pitch_bend))
    
    return midi_note, pitch_bend


# =============================================================================
# MIDI MPE PARSER
# =============================================================================

def parse_mpe_midi(midi_path, speed=1.0):
    """
    Parse an MPE MIDI file into a list of chord events.
    
    Each chord event is a dict:
      {
        'onset_beats': float,   # onset time in beats
        'duration_beats': float, # duration in beats
        'notes': [
          {'step_53': int, 'velocity': int},
          ...
        ]
      }
    
    Args:
        midi_path: Path to MIDI file
        speed: Playback speed multiplier (default 1.0, no change)
    
    Returns:
        list[dict]: List of chord events, sorted by onset time
    """
    midi_path = Path(midi_path)
    mid = mido.MidiFile(midi_path)
    tpb = mid.ticks_per_beat
    
    # We parse from the note track (usually track 1, or the main track)
    # Combine all tracks for safety
    channel_bends = {i: 0 for i in range(16)}  # raw pitch bend values
    active_notes = {}  # (channel, note) → {onset_ticks, step_53, velocity}
    
    # Collect all individual note events first
    note_events = []  # (onset_ticks, offset_ticks, step_53, velocity)
    
    # Parse each track
    for track in mid.tracks:
        abs_time = 0
        for msg in track:
            abs_time += msg.time
            
            if msg.type == "pitchwheel":
                channel_bends[msg.channel] = msg.pitch
                
            elif msg.type == "note_on" and msg.velocity > 0:
                step_53 = midi_bend_to_53tet_step(msg.note, channel_bends.get(msg.channel, 0))
                active_notes[(msg.channel, msg.note)] = {
                    'onset': abs_time,
                    'step_53': step_53,
                    'velocity': msg.velocity
                }
                
            elif msg.type == "note_off" or (msg.type == "note_on" and msg.velocity == 0):
                key = (msg.channel, msg.note)
                if key in active_notes:
                    info = active_notes.pop(key)
                    note_events.append({
                        'onset_ticks': info['onset'],
                        'offset_ticks': abs_time,
                        'step_53': info['step_53'],
                        'velocity': info['velocity']
                    })
    
    if not note_events:
        return []
    
    # Group simultaneous notes into chords
    # Notes with the same onset (within a small tolerance) form a chord
    note_events.sort(key=lambda x: (x['onset_ticks'], x['step_53']))
    
    chords = []
    current_onset = note_events[0]['onset_ticks']
    current_notes = []
    current_offset = 0
    
    TOLERANCE_TICKS = max(1, tpb // 48)  # ~20 ticks tolerance at 960 tpb
    
    for ne in note_events:
        if abs(ne['onset_ticks'] - current_onset) <= TOLERANCE_TICKS:
            current_notes.append(ne)
            current_offset = max(current_offset, ne['offset_ticks'])
        else:
            # Save previous chord
            if current_notes:
                onset_beats = current_onset / tpb / speed
                dur_beats = (current_offset - current_onset) / tpb / speed
                chords.append({
                    'onset_beats': round(onset_beats, 4),
                    'duration_beats': round(max(0.25, dur_beats), 4),
                    'notes': [
                        {'step_53': n['step_53'], 'velocity': n['velocity']}
                        for n in sorted(current_notes, key=lambda x: x['step_53'])
                    ]
                })
            # Start new chord
            current_onset = ne['onset_ticks']
            current_offset = ne['offset_ticks']
            current_notes = [ne]
    
    # Don't forget the last chord
    if current_notes:
        onset_beats = current_onset / tpb / speed
        dur_beats = (current_offset - current_onset) / tpb / speed
        chords.append({
            'onset_beats': round(onset_beats, 4),
            'duration_beats': round(max(0.25, dur_beats), 4),
            'notes': [
                {'step_53': n['step_53'], 'velocity': n['velocity']}
                for n in sorted(current_notes, key=lambda x: x['step_53'])
            ]
        })
    
    return chords


# =============================================================================
# TOKENIZER
# =============================================================================

class MPETokenizer:
    """
    Tokenizer for 53-TET MPE MIDI files.
    
    Converts chord sequences to/from flat token sequences for GPT-2 training.
    
    Token format per chord:
      CHORD_START  DUR_<beats>  P_<step53> V_<bin>  ...  P_<step53> V_<bin>  CHORD_END
    
    Full sequence:
      <start>  [chord tokens...]  BAR  [chord tokens...]  ...  <end>
    
    Attributes:
        token_to_id (dict): Token string → integer ID
        id_to_token (dict): Integer ID → token string
        vocab_size (int): Total vocabulary size
    """
    
    def __init__(self, max_pitch=MAX_53TET_STEP, num_vel_bins=NUM_VELOCITY_BINS, 
                 duration_grid=None, beats_per_bar=4):
        """
        Initialize the tokenizer and build the vocabulary.
        
        Args:
            max_pitch: Maximum 53-TET step to include in vocabulary
            num_vel_bins: Number of velocity quantization bins
            duration_grid: List of allowed durations (beats). Uses default if None.
            beats_per_bar: Beats per bar for BAR token insertion (default 4)
        """
        self.max_pitch = max_pitch
        self.num_vel_bins = num_vel_bins
        self.duration_grid = duration_grid or DURATION_GRID
        self.beats_per_bar = beats_per_bar
        
        # Build vocabulary
        self.token_to_id = {}
        self.id_to_token = {}
        self._build_vocab()
    
    def _build_vocab(self):
        """Construct the full token vocabulary with deterministic ordering."""
        tokens = []
        
        # 1. Special tokens (IDs 0-3)
        tokens.extend([PAD_TOKEN, START_TOKEN, END_TOKEN, SEP_TOKEN])
        
        # 2. Structural tokens
        tokens.extend([CHORD_START_TOKEN, CHORD_END_TOKEN, BAR_TOKEN, REST_TOKEN])
        
        # 3. Duration tokens: DUR_<value>
        for dur in self.duration_grid:
            tokens.append(f"DUR_{dur}")
        
        # 4. Pitch tokens: P_<step> (0 to max_pitch)
        for step in range(self.max_pitch + 1):
            tokens.append(f"P_{step}")
        
        # 5. Velocity tokens: V_<bin> (1 to num_vel_bins)
        for v in range(1, self.num_vel_bins + 1):
            tokens.append(f"V_{v}")
        
        # Build mappings
        self.token_to_id = {tok: i for i, tok in enumerate(tokens)}
        self.id_to_token = {i: tok for i, tok in enumerate(tokens)}
        self.vocab_size = len(tokens)
    
    # -----------------------------------------------------------------
    # Encoding: MIDI → Tokens
    # -----------------------------------------------------------------
    
    def encode_chords(self, chords, add_start_end=True):
        """
        Encode a list of chord events into a flat token sequence.
        
        Args:
            chords: List of chord dicts from parse_mpe_midi()
            add_start_end: Whether to wrap with <start>/<end> tokens
        
        Returns:
            list[str]: Token string sequence
        """
        tokens = []
        
        if add_start_end:
            tokens.append(START_TOKEN)
        
        last_bar = -1  # Track bar lines
        
        for chord in chords:
            # Insert BAR token at bar boundaries
            current_bar = int(chord['onset_beats'] // self.beats_per_bar)
            if current_bar > last_bar:
                # Insert bar markers for each new bar we've entered
                for _ in range(current_bar - max(0, last_bar)):
                    if last_bar >= 0:  # Don't add BAR before the first chord
                        tokens.append(BAR_TOKEN)
                last_bar = current_bar
            
            # Chord start
            tokens.append(CHORD_START_TOKEN)
            
            # Duration (quantized)
            q_dur = quantize_duration(chord['duration_beats'])
            tokens.append(f"DUR_{q_dur}")
            
            # Notes (sorted by pitch, low to high)
            for note in chord['notes'][:MAX_CHORD_NOTES]:
                step = max(0, min(self.max_pitch, note['step_53']))
                vel_bin = quantize_velocity(note['velocity'], self.num_vel_bins)
                tokens.append(f"P_{step}")
                tokens.append(f"V_{vel_bin}")
            
            # Chord end
            tokens.append(CHORD_END_TOKEN)
        
        if add_start_end:
            tokens.append(END_TOKEN)
        
        return tokens
    
    def encode_file(self, midi_path, speed=1.0, add_start_end=True):
        """
        Parse and tokenize a MIDI MPE file.
        
        Args:
            midi_path: Path to MIDI file
            speed: Speed multiplier (applied to timing)
            add_start_end: Wrap with <start>/<end>
        
        Returns:
            list[str]: Token sequence, or empty list on failure
        """
        chords = parse_mpe_midi(midi_path, speed=speed)
        if not chords:
            return []
        return self.encode_chords(chords, add_start_end=add_start_end)
    
    def encode_to_ids(self, tokens):
        """
        Convert token strings to integer IDs.
        
        Args:
            tokens: list[str] — token strings
        
        Returns:
            list[int]: Token IDs
        """
        return [self.token_to_id.get(t, self.token_to_id[PAD_TOKEN]) for t in tokens]
    
    def decode_ids(self, ids):
        """
        Convert integer IDs back to token strings.
        
        Args:
            ids: list[int] — token IDs
        
        Returns:
            list[str]: Token strings
        """
        return [self.id_to_token.get(i, PAD_TOKEN) for i in ids]
    
    # -----------------------------------------------------------------
    # Decoding: Tokens → MIDI events
    # -----------------------------------------------------------------
    
    def decode(self, tokens):
        """
        Decode a token sequence back into chord events.
        
        Reconstructs chord events that can be written to a MIDI file.
        
        Args:
            tokens: list[str] — token sequence
        
        Returns:
            list[dict]: Chord events with 'onset_beats', 'duration_beats', 'notes'
        """
        chords = []
        current_beat = 0.0
        i = 0
        
        while i < len(tokens):
            tok = tokens[i]
            
            if tok == BAR_TOKEN:
                # Advance to next bar boundary
                current_beat = (int(current_beat // self.beats_per_bar) + 1) * self.beats_per_bar
                i += 1
                
            elif tok == CHORD_START_TOKEN:
                i += 1
                duration = 4.0  # default
                notes = []
                
                # Read chord contents until CHORD_END or end of sequence
                while i < len(tokens) and tokens[i] != CHORD_END_TOKEN:
                    t = tokens[i]
                    
                    if t.startswith("DUR_"):
                        duration = float(t[4:])
                    
                    elif t.startswith("P_"):
                        step_53 = int(t[2:])
                        vel = 80  # default
                        # Look ahead for velocity
                        if i + 1 < len(tokens) and tokens[i + 1].startswith("V_"):
                            vel_bin = int(tokens[i + 1][2:])
                            vel = dequantize_velocity(vel_bin, self.num_vel_bins)
                            i += 1  # skip V_ token
                        notes.append({'step_53': step_53, 'velocity': vel})
                    
                    i += 1
                
                if notes:
                    chords.append({
                        'onset_beats': round(current_beat, 4),
                        'duration_beats': duration,
                        'notes': notes
                    })
                    current_beat += duration
                
                i += 1  # skip CHORD_END
            else:
                i += 1
        
        return chords
    
    # -----------------------------------------------------------------
    # MIDI Reconstruction
    # -----------------------------------------------------------------
    
    def chords_to_midi(self, chords, output_path, tpb=960, tempo_bpm=120):
        """
        Write chord events back to an MPE MIDI file.
        
        Args:
            chords: list[dict] — chord events (from decode())
            output_path: Path for the output .mid file
            tpb: Ticks per beat
            tempo_bpm: Tempo in BPM
        """
        mid = mido.MidiFile(type=1, ticks_per_beat=tpb)
        
        # Track 0: tempo + RPN setup
        track0 = mido.MidiTrack()
        mid.tracks.append(track0)
        
        tempo = mido.bpm2tempo(tempo_bpm)
        track0.append(mido.MetaMessage('set_tempo', tempo=tempo, time=0))
        
        # Setup RPN for pitch bend range = 2 semitones on each channel
        for ch in range(1, 16):
            track0.append(mido.Message('control_change', channel=ch, control=101, value=0, time=0))
            track0.append(mido.Message('control_change', channel=ch, control=100, value=0, time=0))
            track0.append(mido.Message('control_change', channel=ch, control=6, value=2, time=0))
            track0.append(mido.Message('control_change', channel=ch, control=38, value=0, time=0))
            track0.append(mido.Message('control_change', channel=ch, control=101, value=127, time=0))
            track0.append(mido.Message('control_change', channel=ch, control=100, value=127, time=0))
        
        # Track 1: notes
        track1 = mido.MidiTrack()
        mid.tracks.append(track1)
        
        # Also set up RPN on track 1
        for ch in range(1, 16):
            track1.append(mido.Message('control_change', channel=ch, control=101, value=0, time=0))
            track1.append(mido.Message('control_change', channel=ch, control=100, value=0, time=0))
            track1.append(mido.Message('control_change', channel=ch, control=6, value=2, time=0))
            track1.append(mido.Message('control_change', channel=ch, control=38, value=0, time=0))
            track1.append(mido.Message('control_change', channel=ch, control=101, value=127, time=0))
            track1.append(mido.Message('control_change', channel=ch, control=100, value=127, time=0))
        
        # Collect all note_on/note_off events with absolute times
        events = []
        channel_pool = list(range(1, 16))  # channels 1-15 for MPE
        
        for chord in chords:
            onset_ticks = int(chord['onset_beats'] * tpb)
            offset_ticks = int((chord['onset_beats'] + chord['duration_beats']) * tpb)
            
            for j, note in enumerate(chord['notes']):
                ch = channel_pool[j % len(channel_pool)]
                midi_note, pitch_bend = step53_to_midi_and_bend(note['step_53'])
                vel = note.get('velocity', 80)
                
                # Pitch bend before note_on
                events.append((onset_ticks, 'pitchwheel', ch, pitch_bend, 0))
                events.append((onset_ticks, 'note_on', ch, midi_note, vel))
                events.append((offset_ticks, 'note_off', ch, midi_note, vel))
        
        # Sort by time, with note_off before note_on at same time, pitchwheel before note_on
        priority = {'pitchwheel': 0, 'note_off': 1, 'note_on': 2}
        events.sort(key=lambda e: (e[0], priority.get(e[1], 1)))
        
        # Convert to delta-time messages
        last_time = 0
        for ev in events:
            abs_time, msg_type, ch, val1, val2 = ev
            delta = abs_time - last_time
            
            if msg_type == 'pitchwheel':
                track1.append(mido.Message('pitchwheel', channel=ch, pitch=val1, time=delta))
            elif msg_type == 'note_on':
                track1.append(mido.Message('note_on', channel=ch, note=val1, velocity=val2, time=delta))
            elif msg_type == 'note_off':
                track1.append(mido.Message('note_off', channel=ch, note=val1, velocity=val2, time=delta))
            
            last_time = abs_time
        
        # Write file
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        mid.save(output_path)
    
    # -----------------------------------------------------------------
    # Padding & Batching
    # -----------------------------------------------------------------
    
    def pad_sequence(self, token_ids, max_length):
        """
        Pad or truncate a token ID sequence to a fixed length.
        
        Args:
            token_ids: list[int] — token IDs
            max_length: Target sequence length
        
        Returns:
            list[int]: Padded/truncated sequence
        """
        pad_id = self.token_to_id[PAD_TOKEN]
        if len(token_ids) >= max_length:
            return token_ids[:max_length]
        return token_ids + [pad_id] * (max_length - len(token_ids))
    
    # -----------------------------------------------------------------
    # Vocabulary I/O
    # -----------------------------------------------------------------
    
    def save_vocab(self, path):
        """Save vocabulary to JSON file."""
        data = {
            'token_to_id': self.token_to_id,
            'config': {
                'max_pitch': self.max_pitch,
                'num_vel_bins': self.num_vel_bins,
                'duration_grid': self.duration_grid,
                'beats_per_bar': self.beats_per_bar,
                'vocab_size': self.vocab_size
            }
        }
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)
    
    @classmethod
    def load_vocab(cls, path):
        """Load tokenizer from a saved vocabulary JSON."""
        with open(path, 'r') as f:
            data = json.load(f)
        config = data['config']
        tokenizer = cls(
            max_pitch=config['max_pitch'],
            num_vel_bins=config['num_vel_bins'],
            duration_grid=config['duration_grid'],
            beats_per_bar=config['beats_per_bar']
        )
        return tokenizer
    
    # -----------------------------------------------------------------
    # Statistics & Debugging
    # -----------------------------------------------------------------
    
    def describe(self):
        """Print a summary of the tokenizer vocabulary."""
        print("=" * 60)
        print("MPE 53-TET Tokenizer — Vocabulary Summary")
        print("=" * 60)
        
        # Count token types
        n_special = 4
        n_structural = 4
        n_duration = len(self.duration_grid)
        n_pitch = self.max_pitch + 1
        n_velocity = self.num_vel_bins
        
        print(f"  Total vocab size:   {self.vocab_size}")
        print(f"  Special tokens:     {n_special}  (IDs 0-{n_special-1})")
        print(f"  Structural tokens:  {n_structural}  ({CHORD_START_TOKEN}, {CHORD_END_TOKEN}, {BAR_TOKEN}, {REST_TOKEN})")
        print(f"  Duration tokens:    {n_duration}  (DUR_{self.duration_grid[0]} .. DUR_{self.duration_grid[-1]})")
        print(f"  Pitch tokens:       {n_pitch}  (P_0 .. P_{self.max_pitch})")
        print(f"  Velocity tokens:    {n_velocity}  (V_1 .. V_{n_velocity})")
        print(f"  Beats per bar:      {self.beats_per_bar}")
        print("=" * 60)
    
    def token_stats(self, tokens):
        """
        Print distribution statistics for a token sequence.
        
        Args:
            tokens: list[str] — token sequence
        """
        counts = Counter(tokens)
        
        # Group by category
        pitches = {k: v for k, v in counts.items() if k.startswith("P_")}
        durations = {k: v for k, v in counts.items() if k.startswith("DUR_")}
        velocities = {k: v for k, v in counts.items() if k.startswith("V_")}
        
        print(f"Total tokens: {len(tokens)}")
        print(f"Unique tokens used: {len(counts)}")
        print(f"Chords: {counts.get(CHORD_START_TOKEN, 0)}")
        print(f"Bars: {counts.get(BAR_TOKEN, 0)}")
        print(f"\nDuration distribution:")
        for k, v in sorted(durations.items(), key=lambda x: -x[1]):
            print(f"  {k}: {v}")
        print(f"\nVelocity distribution:")
        for k, v in sorted(velocities.items(), key=lambda x: -x[1]):
            print(f"  {k}: {v}")
        print(f"\nPitch range: {min(pitches.keys())} — {max(pitches.keys())} ({len(pitches)} unique)")


# =============================================================================
# DATASET CLASS (for GPT-2 training)
# =============================================================================

class MPETokenDataset:
    """
    PyTorch-compatible dataset that tokenizes MIDI MPE files for GPT-2 training.
    
    Each item is a pair (x, y) where:
      x = token_ids[:-1]  (input)
      y = token_ids[1:]   (target, shifted by 1)
    
    This follows the standard autoregressive language model training pattern.
    
    Usage:
        tokenizer = MPETokenizer()
        dataset = MPETokenDataset(
            midi_dir="dataset/midi_files/53_tet_mpe",
            tokenizer=tokenizer,
            block_size=512
        )
        # Use with PyTorch DataLoader
    """
    
    def __init__(self, midi_dir, tokenizer, block_size=512, max_files=None, 
                 file_pattern="*.mid", speed=1.0, verbose=True):
        """
        Args:
            midi_dir: Directory containing MIDI files
            tokenizer: MPETokenizer instance
            block_size: Sequence length for training (context window)
            max_files: Max files to load (None = all)
            file_pattern: Glob pattern for MIDI files
            speed: Speed multiplier for timing
            verbose: Print progress
        """
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.vocab_size = tokenizer.vocab_size
        
        midi_dir = Path(midi_dir)
        files = sorted(midi_dir.glob(file_pattern))
        
        if max_files:
            files = files[:max_files]
        
        if verbose:
            print(f"Loading {len(files)} MIDI files from {midi_dir}...")
        
        # Tokenize all files and collect sequences
        self.sequences = []
        failed = 0
        
        for f in files:
            try:
                tokens = tokenizer.encode_file(f, speed=speed)
                if tokens:
                    ids = tokenizer.encode_to_ids(tokens)
                    self.sequences.append(ids)
            except Exception as e:
                failed += 1
                if verbose and failed <= 5:
                    print(f"  Warning: failed to parse {f.name}: {e}")
        
        if verbose:
            print(f"Successfully tokenized: {len(self.sequences)} files ({failed} failed)")
            lengths = [len(s) for s in self.sequences]
            if lengths:
                print(f"Sequence lengths — min: {min(lengths)}, max: {max(lengths)}, "
                      f"mean: {sum(lengths)/len(lengths):.0f}")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        """
        Returns (x, y) tensors for autoregressive training.
        
        x = padded_ids[:-1]  (input)
        y = padded_ids[1:]   (shifted target)
        """
        import torch
        
        ids = self.tokenizer.pad_sequence(self.sequences[idx], self.block_size + 1)
        ids = torch.tensor(ids, dtype=torch.long)
        
        x = ids[:-1]  # input:  [0, 1, 2, ..., block_size-1]
        y = ids[1:]   # target: [1, 2, 3, ..., block_size]
        
        return x, y


# =============================================================================
# BATCH PROCESSING
# =============================================================================

def tokenize_dataset(midi_dir, output_dir, tokenizer=None, max_files=None, speed=1.0):
    """
    Batch-tokenize all MIDI files in a directory and save as JSON.
    
    Saves:
      - <output_dir>/tokenized_sequences.json  (all token ID sequences)
      - <output_dir>/vocab.json                 (tokenizer vocabulary)
      - <output_dir>/stats.json                 (dataset statistics)
    
    Args:
        midi_dir: Input directory with .mid files
        output_dir: Output directory
        tokenizer: MPETokenizer (creates default if None)
        max_files: Max files to process
        speed: Speed multiplier
    
    Returns:
        tuple: (tokenizer, sequences, stats)
    """
    if tokenizer is None:
        tokenizer = MPETokenizer()
    
    midi_dir = Path(midi_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    files = sorted(midi_dir.glob("*.mid"))
    if max_files:
        files = files[:max_files]
    
    print(f"Tokenizing {len(files)} files...")
    
    sequences = []
    all_tokens = []
    failed = 0
    
    for i, f in enumerate(files):
        try:
            tokens = tokenizer.encode_file(f, speed=speed)
            if tokens:
                ids = tokenizer.encode_to_ids(tokens)
                sequences.append({
                    'file': f.name,
                    'token_ids': ids,
                    'length': len(ids)
                })
                all_tokens.extend(tokens)
        except Exception as e:
            failed += 1
        
        if (i + 1) % 1000 == 0:
            print(f"  Processed {i + 1}/{len(files)} ({failed} failed)")
    
    print(f"Done. {len(sequences)} sequences, {failed} failures")
    
    # Statistics
    lengths = [s['length'] for s in sequences]
    token_counts = Counter(all_tokens)
    
    stats = {
        'total_files': len(files),
        'successful': len(sequences),
        'failed': failed,
        'total_tokens': len(all_tokens),
        'unique_tokens_used': len(token_counts),
        'vocab_size': tokenizer.vocab_size,
        'seq_length_min': min(lengths) if lengths else 0,
        'seq_length_max': max(lengths) if lengths else 0,
        'seq_length_mean': round(sum(lengths) / len(lengths), 1) if lengths else 0,
        'top_20_tokens': token_counts.most_common(20)
    }
    
    # Save outputs
    tokenizer.save_vocab(output_dir / "vocab.json")
    
    with open(output_dir / "stats.json", 'w') as f:
        json.dump(stats, f, indent=2)
    
    # Save sequences (token IDs only, for efficiency)
    seq_data = [{'file': s['file'], 'ids': s['token_ids']} for s in sequences]
    with open(output_dir / "tokenized_sequences.json", 'w') as f:
        json.dump(seq_data, f)
    
    print(f"\nSaved to {output_dir}/")
    print(f"  vocab.json ({tokenizer.vocab_size} tokens)")
    print(f"  tokenized_sequences.json ({len(sequences)} sequences)")
    print(f"  stats.json")
    
    return tokenizer, sequences, stats


# =============================================================================
# ROUNDTRIP VERIFICATION
# =============================================================================

def verify_roundtrip(midi_path, output_path=None, tokenizer=None, verbose=True):
    """
    Verify encode → decode → MIDI roundtrip for a single file.
    
    Encodes a MIDI file to tokens, decodes back to chords, and optionally
    writes a new MIDI file. Reports any discrepancies.
    
    Args:
        midi_path: Path to source MIDI file
        output_path: Path for reconstructed MIDI (optional)
        tokenizer: MPETokenizer (creates default if None)
        verbose: Print details
    
    Returns:
        dict: Comparison report
    """
    if tokenizer is None:
        tokenizer = MPETokenizer()
    
    # Step 1: Parse original
    original_chords = parse_mpe_midi(midi_path)
    
    # Step 2: Encode
    tokens = tokenizer.encode_chords(original_chords)
    token_ids = tokenizer.encode_to_ids(tokens)
    
    # Step 3: Decode back
    decoded_tokens = tokenizer.decode_ids(token_ids)
    reconstructed_chords = tokenizer.decode(decoded_tokens)
    
    # Step 4: Compare
    report = {
        'file': str(midi_path),
        'original_chords': len(original_chords),
        'reconstructed_chords': len(reconstructed_chords),
        'token_count': len(tokens),
        'match': len(original_chords) == len(reconstructed_chords)
    }
    
    if verbose:
        print(f"Roundtrip: {Path(midi_path).name}")
        print(f"  Original chords:      {len(original_chords)}")
        print(f"  Token sequence length: {len(tokens)}")
        print(f"  Reconstructed chords:  {len(reconstructed_chords)}")
        
        # Compare pitch content
        if original_chords and reconstructed_chords:
            n_compare = min(len(original_chords), len(reconstructed_chords))
            pitch_matches = 0
            for i in range(n_compare):
                orig_pitches = sorted([n['step_53'] for n in original_chords[i]['notes']])
                recon_pitches = sorted([n['step_53'] for n in reconstructed_chords[i]['notes']])
                if orig_pitches == recon_pitches:
                    pitch_matches += 1
            
            accuracy = pitch_matches / n_compare * 100
            report['pitch_accuracy'] = round(accuracy, 1)
            print(f"  Pitch accuracy:       {accuracy:.1f}% ({pitch_matches}/{n_compare})")
    
    # Step 5: Write reconstructed MIDI
    if output_path and reconstructed_chords:
        tokenizer.chords_to_midi(reconstructed_chords, output_path)
        if verbose:
            print(f"  Saved: {output_path}")
    
    return report


# =============================================================================
# CLI
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="53-TET MPE MIDI Tokenizer")
    subparsers = parser.add_subparsers(dest="command")
    
    # Describe vocabulary
    sub = subparsers.add_parser("describe", help="Show tokenizer vocabulary info")
    
    # Encode a single file
    sub = subparsers.add_parser("encode", help="Tokenize a single MIDI file")
    sub.add_argument("midi_file", help="Path to MIDI file")
    sub.add_argument("--stats", action="store_true", help="Show token statistics")
    
    # Verify roundtrip
    sub = subparsers.add_parser("verify", help="Verify encode/decode roundtrip")
    sub.add_argument("midi_file", help="Path to MIDI file")
    sub.add_argument("--output", "-o", help="Output MIDI path for comparison")
    
    # Batch tokenize
    sub = subparsers.add_parser("batch", help="Batch tokenize a directory")
    sub.add_argument("midi_dir", help="Directory with MIDI files")
    sub.add_argument("output_dir", help="Output directory for tokenized data")
    sub.add_argument("--max-files", type=int, help="Max files to process")
    
    args = parser.parse_args()
    tokenizer = MPETokenizer()
    
    if args.command == "describe":
        tokenizer.describe()
    
    elif args.command == "encode":
        tokens = tokenizer.encode_file(args.midi_file)
        if tokens:
            print(f"Tokens ({len(tokens)}):")
            print(" ".join(tokens[:80]))
            if len(tokens) > 80:
                print(f"  ... ({len(tokens) - 80} more)")
            if args.stats:
                print()
                tokenizer.token_stats(tokens)
        else:
            print("Failed to tokenize file.")
    
    elif args.command == "verify":
        verify_roundtrip(args.midi_file, args.output, tokenizer)
    
    elif args.command == "batch":
        tokenize_dataset(args.midi_dir, args.output_dir, tokenizer, args.max_files)
    
    else:
        parser.print_help()
